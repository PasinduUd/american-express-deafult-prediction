{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_feather('../input/amexfeather/train_data.ftr')\nprint(\"Train Dataset : Rows =\", train_df.shape[0], \", Columns = \", train_df.shape[1])\ntrain_df = train_df.set_index('customer_ID', drop=True)\nprint(\"Train Dataset : Rows =\", train_df.shape[0], \", Columns = \", train_df.shape[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_feature_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\nnumerical_feature_cols = [col for col in train_df.columns if col not in categorical_feature_cols + [\"target\"]]\nnumerical_feature_cols.remove('S_2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns which contain null values > 80%\nremovable_feature_cols = np.array(['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'])\n\ntrain_df = train_df.drop(removable_feature_cols, axis=1)\nprint(\"Train Dataset : Rows =\", train_df.shape[0], \", Columns = \", train_df.shape[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_feature_cols_w_NaN = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])\n\nfor col in numerical_feature_cols_w_NaN:\n    train_df[col] = train_df[col].fillna(train_df[col].median())\n    \ncategorical_feature_cols_w_NaN = np.array(['D_68','B_30','B_38','D_64','D_114','D_116','D_117','D_120','D_126'])\n\nfor col in categorical_feature_cols_w_NaN:\n    train_df[col] =  train_df[col].fillna(train_df[col].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df.isnull().sum().to_string()) # Check the existence of NaN values\nX_train = train_df.iloc[:, :-1]\nprint(\"X : Rows =\", X_train.shape[0], \", Columns = \", X_train.shape[1])\ny_train = train_df.iloc[:, -1:]\nprint(\"y : Rows =\", y_train.shape[0], \", Columns = \", y_train.shape[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ncategorical_feature_cols.remove(\"D_66\")\nfor col in categorical_feature_cols:\n    X_train[col] = label_encoder.fit_transform(X_train[col])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.groupby('customer_ID').mean()\n\nfor col in categorical_feature_cols:\n    X_train[col] = X_train[col].round(0).astype(int)\n    \ny_train = y_train.groupby('customer_ID').mean()\ny_train = y_train.round(0).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_feather('../input/amexfeather/test_data.ftr')\nprint(\"Test Dataset : Rows =\", test_df.shape[0], \", Columns = \", test_df.shape[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.set_index('customer_ID', drop=True)\nremovable_feature_cols = ['S_2','D_42','D_49','D_66','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142']\nnumerical_feature_cols_w_NaN = ['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','S_12','S_17','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145']\ncategorical_feature_cols_w_NaN = ['D_68','B_30','B_38','D_114','D_116','D_117','D_120','D_126']\ntest_df = test_df.drop(removable_feature_cols, axis=1)\nfor col in numerical_feature_cols_w_NaN:\n    test_df[col] = test_df[col].fillna(test_df[col].median())\nfor col in categorical_feature_cols_w_NaN:\n    test_df[col] =  test_df[col].fillna(test_df[col].mode()[0])\nprint(\"Test Dataset : Rows =\", test_df.shape[0], \", Columns = \", test_df.shape[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(test_df.isnull().sum().to_string())\nfor col in categorical_feature_cols:\n    test_df[col] = label_encoder.fit_transform(test_df[col])\ntest_df = test_df.groupby('customer_ID').mean()\nfor col in categorical_feature_cols:\n    test_df[col] = test_df[col].round(0).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove columns if there are > 90% of correlations\ncorrelation_matrix = X_train.corr()\ncol_core = set()\n\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if(correlation_matrix.iloc[i, j] > 0.9):\n            col = correlation_matrix.columns[i]\n            col_core.add(col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(col_core, axis=1)\ntest_df = test_df.drop(col_core, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model = svm.SVC(kernel='linear') # Linear Kernel\nsvm_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = svm_model.predict(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = pd.DataFrame(predictions, columns = [\"prediction\"])\ndf_ids = test_df.index.to_frame()\ndf_ids = df_ids.reset_index(drop=True)\ndf = pd.concat([df_ids, df_predictions], axis=1)\ndf.to_csv(\"SVM_predictions.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}